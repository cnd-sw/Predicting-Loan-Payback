{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nimport warnings\nimport os\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom itertools import combinations\nfrom scipy.optimize import minimize\n\nwarnings.simplefilter('ignore')\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\nTRAIN_PATH = '/kaggle/input/playground-series-s5e11/train.csv'\nTEST_PATH = '/kaggle/input/playground-series-s5e11/test.csv'\nORIG_PATH = '/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv' \nSUBMISSION_PATH = 'submission.csv'\nTARGET = 'loan_paid_back'\nN_SPLITS = 10\nSEED = 42\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Target Encoder with smoothing and CV handling.\n    \"\"\"\n    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth='auto', drop_original=False):\n        self.cols_to_encode = cols_to_encode\n        self.aggs = aggs\n        self.cv = cv\n        self.smooth = smooth\n        self.drop_original = drop_original\n        self.mappings_ = {}\n        self.global_stats_ = {}\n\n    def fit(self, X, y):\n        temp_df = X.copy()\n        temp_df['target'] = y\n        for agg_func in self.aggs:\n            self.global_stats_[agg_func] = y.agg(agg_func)\n        for col in self.cols_to_encode:\n            self.mappings_[col] = {}\n            for agg_func in self.aggs:\n                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n                self.mappings_[col][agg_func] = mapping\n        return self\n\n    def transform(self, X):\n        X_transformed = X.copy()\n        for col in self.cols_to_encode:\n            for agg_func in self.aggs:\n                new_col_name = f'TE_{col}_{agg_func}'\n                if col in self.mappings_ and agg_func in self.mappings_[col]:\n                    map_series = self.mappings_[col][agg_func]\n                    X_transformed[new_col_name] = X[col].map(map_series)\n                    X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n                else:\n                    X_transformed[new_col_name] = self.global_stats_[agg_func]\n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n        return X_transformed\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        encoded_features = pd.DataFrame(index=X.index)\n        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n        for train_idx, val_idx in kf.split(X, y):\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val = X.iloc[val_idx]\n            temp_df_train = X_train.copy()\n            temp_df_train['target'] = y_train\n            for col in self.cols_to_encode:\n                for agg_func in self.aggs:\n                    new_col_name = f'TE_{col}_{agg_func}'\n                    fold_global_stat = y_train.agg(agg_func)\n                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n                    if agg_func == 'mean':\n                        counts = temp_df_train.groupby(col)['target'].count()\n                        m = self.smooth\n                        if self.smooth == 'auto':\n                            variance_between = mapping.var()\n                            avg_variance_within = temp_df_train.groupby(col)['target'].var().mean()\n                            if variance_between > 0:\n                                m = avg_variance_within / variance_between\n                            else:\n                                m = 0\n                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n                        encoded_values = X_val[col].map(smoothed_mapping)\n                    else:\n                        encoded_values = X_val[col].map(mapping)\n                    encoded_features.loc[X_val.index, new_col_name] = encoded_values.fillna(fold_global_stat)\n        X_transformed = X.copy()\n        for col in encoded_features.columns:\n            X_transformed[col] = encoded_features[col]\n        if self.drop_original:\n            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n        return X_transformed\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train, test, orig=None):\n    print(\"Starting Feature Engineering...\")\n    \n    # Base columns\n    CATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n    BASE = [col for col in train.columns if col not in ['id', TARGET]]\n    \n    # Combined df for consistent processing\n    train['is_train'] = 1\n    test['is_train'] = 0\n    test[TARGET] = np.nan\n    \n    if orig is not None:\n        orig['is_train'] = 1\n        combined = pd.concat([train, test, orig], axis=0, ignore_index=True)\n    else:\n        combined = pd.concat([train, test], axis=0, ignore_index=True)\n        \n    # 1. Log Transforms (New)\n    print(\"Creating Log Features...\")\n    for col in ['annual_income', 'loan_amount']:\n        combined[f'log_{col}'] = np.log1p(combined[col])\n        \n    # 2. Ratio Features (New)\n    print(\"Creating Ratio Features...\")\n    combined['loan_to_income'] = combined['loan_amount'] / (combined['annual_income'] + 1)\n    combined['monthly_debt_est'] = (combined['annual_income'] / 12) * combined['debt_to_income_ratio']\n    combined['total_interest_est'] = combined['loan_amount'] * (combined['interest_rate'] / 100)\n    \n    # 3. Digit Features\n    print(\"Creating Digit Features...\")\n    cols_to_digitize = {\n        'debt_to_income_ratio': 1000,\n        'credit_score': 'direct',\n        'interest_rate': 100,\n    }\n    \n    DIGIT = []\n    for col, multiplier in cols_to_digitize.items():\n        temp_col_name = f'{col}_TEMP_INT'\n        if multiplier == 'direct':\n            combined[temp_col_name] = combined[col]\n        else:\n            combined[temp_col_name] = (combined[col] * multiplier).round(0).astype(int)\n        \n        temp_str = combined[temp_col_name].astype(str)\n        if col == 'credit_score': max_len = 3\n        elif col == 'debt_to_income_ratio': max_len = 3\n        elif col == 'interest_rate': max_len = 4\n        \n        temp_str_padded = temp_str.str.zfill(max_len)\n        for i in range(max_len):\n            new_col_name = f'{col}_DIGIT_{i+1}'\n            DIGIT.append(new_col_name)\n            combined[new_col_name] = temp_str_padded.str[i].replace('.', '0').replace('-', '0').astype(int)\n            \n        combined.drop(columns=[temp_col_name], inplace=True)\n\n    # 4. Round Features\n    print(\"Creating Round Features...\")\n    ROUND = []\n    rounding_levels = {'1s': 0, '10s': -1, '100s': -2, '1000s': -3}\n    for col in ['annual_income', 'loan_amount']:\n        for suffix, level in rounding_levels.items():\n            new_col_name = f'{col}_ROUND_{suffix}'\n            ROUND.append(new_col_name)\n            combined[new_col_name] = combined[col].round(level).fillna(0).astype(int)\n\n    # 5. Interaction Features\n    print(\"Creating Interaction Features...\")\n    INTER = []\n    INTER_BASE = CATS + ['credit_score'] \n    for col1, col2 in combinations(INTER_BASE, 2):\n        new_col_name = f'{col1}_{col2}'\n        INTER.append(new_col_name)\n        combined[new_col_name] = combined[col1].astype(str) + '_' + combined[col2].astype(str)\n\n    # Split back\n    train_processed = combined[combined['is_train'] == 1].copy()\n    test_processed = combined[combined['is_train'] == 0].copy()\n    \n    train_processed.drop(columns=['is_train'], inplace=True)\n    test_processed.drop(columns=['is_train', TARGET], inplace=True)\n    \n    NEW_NUMERICS = ['log_annual_income', 'log_loan_amount', 'loan_to_income', 'monthly_debt_est', 'total_interest_est']\n    FEATURES = BASE + NEW_NUMERICS + INTER + ROUND + DIGIT\n    FEATURES = list(set(FEATURES))\n    FEATURES = [f for f in FEATURES if f in train_processed.columns]\n    \n    print(f\"Total Features: {len(FEATURES)}\")\n    \n    return train_processed, test_processed, FEATURES, CATS, INTER, ROUND, DIGIT\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train, test, FEATURES, CATS, INTER, ROUND, DIGIT):\n    X = train[FEATURES]\n    y = train[TARGET]\n    X_test_final = test[FEATURES].copy()\n    \n    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n    \n    oof_preds_xgb = np.zeros(len(X))\n    test_preds_xgb = np.zeros(len(test))\n    \n    oof_preds_lgb = np.zeros(len(X))\n    test_preds_lgb = np.zeros(len(test))\n    \n    oof_preds_cat = np.zeros(len(X))\n    test_preds_cat = np.zeros(len(test))\n    \n    # Enhanced Parameters\n    xgb_params = {\n        'n_estimators': 3000,\n        'learning_rate': 0.005,\n        'max_depth': 8,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'n_jobs': -1,\n        'random_state': SEED,\n        'tree_method': 'hist',\n        'early_stopping_rounds': 200,\n        'eval_metric': 'auc',\n        'reg_alpha': 0.1,\n        'reg_lambda': 1.0\n    }\n    \n    lgb_params = {\n        'n_estimators': 3000,\n        'learning_rate': 0.005,\n        'max_depth': 10,\n        'num_leaves': 64,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'n_jobs': -1,\n        'random_state': SEED,\n        'metric': 'auc',\n        'verbosity': -1,\n        'reg_alpha': 0.1,\n        'reg_lambda': 1.0\n    }\n    \n    cat_params = {\n        'iterations': 3000,\n        'learning_rate': 0.005,\n        'depth': 8,\n        'l2_leaf_reg': 5,\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'random_seed': SEED,\n        'verbose': 0,\n        'early_stopping_rounds': 200,\n        'task_type': 'CPU'\n    }\n\n    print(\"Starting Training...\")\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n        print(f'--- Fold {fold}/{N_SPLITS} ---')\n        \n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        X_test_fold = X_test_final.copy()\n        \n        # Target Encoding inside fold\n        TE_INTER = TargetEncoder(cols_to_encode=INTER, cv=5, smooth='auto', aggs=['mean'], drop_original=True)\n        X_train = TE_INTER.fit_transform(X_train, y_train)\n        X_val = TE_INTER.transform(X_val)\n        X_test_fold = TE_INTER.transform(X_test_fold)\n        \n        cols_to_encode_base = ['debt_to_income_ratio', 'credit_score'] + ROUND + DIGIT\n        cols_to_encode_base = [c for c in cols_to_encode_base if c in X_train.columns]\n        \n        TE_BASE = TargetEncoder(cols_to_encode=cols_to_encode_base, cv=5, smooth='auto', aggs=['mean'], drop_original=False)\n        X_train = TE_BASE.fit_transform(X_train, y_train)\n        X_val = TE_BASE.transform(X_val)\n        X_test_fold = TE_BASE.transform(X_test_fold)\n        \n        for c in CATS:\n            if c in X_train.columns:\n                combined = pd.concat([X_train[c], X_val[c], X_test_fold[c]])\n                combined_codes, _ = combined.factorize()\n                X_train[c] = combined_codes[:len(X_train)]\n                X_val[c] = combined_codes[len(X_train):len(X_train)+len(X_val)]\n                X_test_fold[c] = combined_codes[len(X_train)+len(X_val):]\n                \n                X_train[c] = X_train[c].astype('category')\n                X_val[c] = X_val[c].astype('category')\n                X_test_fold[c] = X_test_fold[c].astype('category')\n\n        # XGBoost\n        xgb = XGBClassifier(**xgb_params, enable_categorical=True)\n        xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n        val_pred_xgb = xgb.predict_proba(X_val)[:, 1]\n        oof_preds_xgb[val_idx] = val_pred_xgb\n        test_preds_xgb += xgb.predict_proba(X_test_fold)[:, 1] / N_SPLITS\n        print(f\"XGB AUC: {roc_auc_score(y_val, val_pred_xgb):.5f}\")\n        \n        # LightGBM\n        lgb = LGBMClassifier(**lgb_params)\n        lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[])\n        val_pred_lgb = lgb.predict_proba(X_val)[:, 1]\n        oof_preds_lgb[val_idx] = val_pred_lgb\n        test_preds_lgb += lgb.predict_proba(X_test_fold)[:, 1] / N_SPLITS\n        print(f\"LGB AUC: {roc_auc_score(y_val, val_pred_lgb):.5f}\")\n        \n        # CatBoost\n        cat_features_indices = [c for c in CATS if c in X_train.columns]\n        cat = CatBoostClassifier(**cat_params)\n        cat.fit(X_train, y_train, eval_set=(X_val, y_val), cat_features=cat_features_indices)\n        val_pred_cat = cat.predict_proba(X_val)[:, 1]\n        oof_preds_cat[val_idx] = val_pred_cat\n        test_preds_cat += cat.predict_proba(X_test_fold)[:, 1] / N_SPLITS\n        print(f\"CAT AUC: {roc_auc_score(y_val, val_pred_cat):.5f}\")\n        \n    return oof_preds_xgb, test_preds_xgb, oof_preds_lgb, test_preds_lgb, oof_preds_cat, test_preds_cat, y\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ensemble_weights(oof_preds_list, y_true):\n    \"\"\"\n    Find optimal weights for the ensemble using SLSQP.\n    \"\"\"\n    print(\"Optimizing ensemble weights...\")\n    \n    def loss_func(weights):\n        final_pred = np.zeros_like(oof_preds_list[0])\n        for i, pred in enumerate(oof_preds_list):\n            final_pred += weights[i] * pred\n        return -roc_auc_score(y_true, final_pred)\n    \n    starting_values = [1/len(oof_preds_list)] * len(oof_preds_list)\n    constraints = ({'type': 'eq', 'fun': lambda w: 1 - sum(w)})\n    bounds = [(0, 1)] * len(oof_preds_list)\n    \n    res = minimize(loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=constraints)\n    \n    print(f\"Optimal Weights: {res.x}\")\n    print(f\"Optimized AUC: {-res.fun:.5f}\")\n    return res.x\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\nprint(\"Loading data...\")\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\n\nif os.path.exists(ORIG_PATH):\n    print(f\"Found original dataset at {ORIG_PATH}\")\n    orig = pd.read_csv(ORIG_PATH)\nelse:\n    print(\"Original dataset not found. Proceeding without it.\")\n    orig = None\n    \n# 2. Feature Engineering\ntrain, test, FEATURES, CATS, INTER, ROUND, DIGIT = feature_engineering(train, test, orig)\n\n# 3. Train Models\noof_xgb, pred_xgb, oof_lgb, pred_lgb, oof_cat, pred_cat, y = train_models(train, test, FEATURES, CATS, INTER, ROUND, DIGIT)\n\n# 4. Ensemble Optimization\nweights = optimize_ensemble_weights([oof_xgb, oof_lgb, oof_cat], y)\n\nfinal_preds = (pred_xgb * weights[0] + pred_lgb * weights[1] + pred_cat * weights[2])\n\n# 5. Submission\nsubmission = pd.DataFrame({'id': test['id'], TARGET: final_preds})\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Submission saved to {SUBMISSION_PATH}\")\\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from itertools import combinations\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Configuration\n",
    "# NOTE: Update these paths if the dataset directory name is different in your Kaggle input\n",
    "TRAIN_PATH = '/kaggle/input/playground-series-s5e11/train.csv'\n",
    "TEST_PATH = '/kaggle/input/playground-series-s5e11/test.csv'\n",
    "# Optional: Add the original dataset if you have it uploaded\n",
    "ORIG_PATH = '/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv' \n",
    "SUBMISSION_PATH = 'submission.csv'\n",
    "TARGET = 'loan_paid_back'\n",
    "N_SPLITS = 10 \n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Target Encoder from existing solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth='auto', drop_original=False):\n",
    "        self.cols_to_encode = cols_to_encode\n",
    "        self.aggs = aggs\n",
    "        self.cv = cv\n",
    "        self.smooth = smooth\n",
    "        self.drop_original = drop_original\n",
    "        self.mappings_ = {}\n",
    "        self.global_stats_ = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        temp_df = X.copy()\n",
    "        temp_df['target'] = y\n",
    "        for agg_func in self.aggs:\n",
    "            self.global_stats_[agg_func] = y.agg(agg_func)\n",
    "        for col in self.cols_to_encode:\n",
    "            self.mappings_[col] = {}\n",
    "            for agg_func in self.aggs:\n",
    "                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n",
    "                self.mappings_[col][agg_func] = mapping\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.cols_to_encode:\n",
    "            for agg_func in self.aggs:\n",
    "                new_col_name = f'TE_{col}_{agg_func}'\n",
    "                map_series = self.mappings_[col][agg_func]\n",
    "                X_transformed[new_col_name] = X[col].map(map_series)\n",
    "                X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n",
    "        if self.drop_original:\n",
    "            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        encoded_features = pd.DataFrame(index=X.index)\n",
    "        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n",
    "        for train_idx, val_idx in kf.split(X, y):\n",
    "            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            temp_df_train = X_train.copy()\n",
    "            temp_df_train['target'] = y_train\n",
    "            for col in self.cols_to_encode:\n",
    "                for agg_func in self.aggs:\n",
    "                    new_col_name = f'TE_{col}_{agg_func}'\n",
    "                    fold_global_stat = y_train.agg(agg_func)\n",
    "                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n",
    "                    if agg_func == 'mean':\n",
    "                        counts = temp_df_train.groupby(col)['target'].count()\n",
    "                        m = self.smooth\n",
    "                        if self.smooth == 'auto':\n",
    "                            variance_between = mapping.var()\n",
    "                            avg_variance_within = temp_df_train.groupby(col)['target'].var().mean()\n",
    "                            if variance_between > 0:\n",
    "                                m = avg_variance_within / variance_between\n",
    "                            else:\n",
    "                                m = 0\n",
    "                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n",
    "                        encoded_values = X_val[col].map(smoothed_mapping)\n",
    "                    else:\n",
    "                        encoded_values = X_val[col].map(mapping)\n",
    "                    encoded_features.loc[X_val.index, new_col_name] = encoded_values.fillna(fold_global_stat)\n",
    "        X_transformed = X.copy()\n",
    "        for col in encoded_features.columns:\n",
    "            X_transformed[col] = encoded_features[col]\n",
    "        if self.drop_original:\n",
    "            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train, test, orig=None):\n",
    "    print(\"Starting Feature Engineering...\")\n",
    "    \n",
    "    # Base columns\n",
    "    CATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
    "    BASE = [col for col in train.columns if col not in ['id', TARGET]]\n",
    "    \n",
    "    # 1. Digit Features\n",
    "    print(\"Creating Digit Features...\")\n",
    "    cols_to_digitize = {\n",
    "        'debt_to_income_ratio': 1000,\n",
    "        'credit_score': 'direct',\n",
    "        'interest_rate': 100,\n",
    "    }\n",
    "    \n",
    "    DIGIT = []\n",
    "    dfs = [train, test]\n",
    "    if orig is not None: dfs.append(orig)\n",
    "    \n",
    "    for col, multiplier in cols_to_digitize.items():\n",
    "        temp_col_name = f'{col}_TEMP_INT'\n",
    "        for df in dfs:\n",
    "            if multiplier == 'direct':\n",
    "                df[temp_col_name] = df[col]\n",
    "            else:\n",
    "                df[temp_col_name] = (df[col] * multiplier).round(0).astype(int)\n",
    "            \n",
    "            temp_str = df[temp_col_name].astype(str)\n",
    "            if col == 'credit_score': max_len = 3\n",
    "            elif col == 'debt_to_income_ratio': max_len = 3\n",
    "            elif col == 'interest_rate': max_len = 4\n",
    "            \n",
    "            temp_str_padded = temp_str.str.zfill(max_len)\n",
    "            for i in range(max_len):\n",
    "                new_col_name = f'{col}_DIGIT_{i+1}'\n",
    "                if df is train: DIGIT.append(new_col_name)\n",
    "                df[new_col_name] = temp_str_padded.str[i].astype(int)\n",
    "        \n",
    "        for df in dfs:\n",
    "            df.drop(columns=[temp_col_name], inplace=True)\n",
    "\n",
    "    # 2. Round Features\n",
    "    print(\"Creating Round Features...\")\n",
    "    ROUND = []\n",
    "    rounding_levels = {'1s': 0, '10s': -1, '100s': -2, '1000s': -3}\n",
    "    for col in ['annual_income', 'loan_amount']:\n",
    "        for suffix, level in rounding_levels.items():\n",
    "            new_col_name = f'{col}_ROUND_{suffix}'\n",
    "            ROUND.append(new_col_name)\n",
    "            for df in dfs:\n",
    "                df[new_col_name] = df[col].round(level).astype(int)\n",
    "\n",
    "    # 3. Interaction Features\n",
    "    print(\"Creating Interaction Features...\")\n",
    "    INTER = []\n",
    "    for col1, col2 in combinations(BASE, 2):\n",
    "        new_col_name = f'{col1}_{col2}'\n",
    "        INTER.append(new_col_name)\n",
    "        for df in dfs:\n",
    "            df[new_col_name] = df[col1].astype(str) + '_' + df[col2].astype(str)\n",
    "\n",
    "    # 4. Original Data Features (if available)\n",
    "    ORIG_FEATS = []\n",
    "    if orig is not None:\n",
    "        print(\"Creating Original Data Features...\")\n",
    "        for col in BASE:\n",
    "            # Mean\n",
    "            mean_map = orig.groupby(col)[TARGET].mean()\n",
    "            new_mean_col_name = f\"orig_mean_{col}\"\n",
    "            mean_map.name = new_mean_col_name\n",
    "            train = train.merge(mean_map, on=col, how='left')\n",
    "            test = test.merge(mean_map, on=col, how='left')\n",
    "            ORIG_FEATS.append(new_mean_col_name)\n",
    "            \n",
    "            # Count\n",
    "            new_count_col_name = f\"orig_count_{col}\"\n",
    "            count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n",
    "            train = train.merge(count_map, on=col, how='left')\n",
    "            test = test.merge(count_map, on=col, how='left')\n",
    "            ORIG_FEATS.append(new_count_col_name)\n",
    "            \n",
    "        train[ORIG_FEATS] = train[ORIG_FEATS].fillna(orig[TARGET].mean())\n",
    "        test[ORIG_FEATS] = test[ORIG_FEATS].fillna(orig[TARGET].mean())\n",
    "\n",
    "    # Feature Lists\n",
    "    FEATURES = BASE + ORIG_FEATS + INTER + ROUND + DIGIT\n",
    "    print(f\"Total Features: {len(FEATURES)}\")\n",
    "    \n",
    "    return train, test, FEATURES, CATS, INTER, ROUND, DIGIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train, test, FEATURES, CATS, INTER, ROUND, DIGIT):\n",
    "    X = train[FEATURES]\n",
    "    y = train[TARGET]\n",
    "    X_test_final = test[FEATURES].copy()\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    oof_preds_xgb = np.zeros(len(X))\n",
    "    test_preds_xgb = np.zeros(len(test))\n",
    "    \n",
    "    oof_preds_lgb = np.zeros(len(X))\n",
    "    test_preds_lgb = np.zeros(len(test))\n",
    "    \n",
    "    oof_preds_cat = np.zeros(len(X))\n",
    "    test_preds_cat = np.zeros(len(test))\n",
    "    \n",
    "    # Model Parameters\n",
    "    xgb_params = {\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': SEED,\n",
    "        'tree_method': 'hist',\n",
    "        'early_stopping_rounds': 100,\n",
    "        'eval_metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    lgb_params = {\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 8,\n",
    "        'num_leaves': 32,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': SEED,\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1\n",
    "    }\n",
    "    \n",
    "    cat_params = {\n",
    "        'iterations': 2000,\n",
    "        'learning_rate': 0.01,\n",
    "        'depth': 6,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'AUC',\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'early_stopping_rounds': 100\n",
    "    }\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f'--- Fold {fold}/{N_SPLITS} ---')\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        X_test_fold = X_test_final.copy()\n",
    "        \n",
    "        # Target Encoding inside fold\n",
    "        # Encode Interactions\n",
    "        TE_INTER = TargetEncoder(cols_to_encode=INTER, cv=5, smooth='auto', aggs=['mean'], drop_original=True)\n",
    "        X_train = TE_INTER.fit_transform(X_train, y_train)\n",
    "        X_val = TE_INTER.transform(X_val)\n",
    "        X_test_fold = TE_INTER.transform(X_test_fold)\n",
    "        \n",
    "        # Encode Base + Round + Digit\n",
    "        BASE_TE_COL = ['debt_to_income_ratio', 'credit_score'] + ROUND + DIGIT\n",
    "        TE_BASE = TargetEncoder(cols_to_encode=BASE_TE_COL, cv=5, smooth='auto', aggs=['mean'], drop_original=False)\n",
    "        X_train = TE_BASE.fit_transform(X_train, y_train)\n",
    "        X_val = TE_BASE.transform(X_val)\n",
    "        X_test_fold = TE_BASE.transform(X_test_fold)\n",
    "        \n",
    "        # Factorize Categoricals\n",
    "        for c in CATS:\n",
    "            combined = pd.concat([X_train[c], X_val[c], X_test_fold[c]])\n",
    "            combined_codes, _ = combined.factorize()\n",
    "            X_train[c] = combined_codes[:len(X_train)]\n",
    "            X_val[c] = combined_codes[len(X_train):len(X_train)+len(X_val)]\n",
    "            X_test_fold[c] = combined_codes[len(X_train)+len(X_val):]\n",
    "            \n",
    "            # Ensure categorical type for LGBM\n",
    "            X_train[c] = X_train[c].astype('category')\n",
    "            X_val[c] = X_val[c].astype('category')\n",
    "            X_test_fold[c] = X_test_fold[c].astype('category')\n",
    "\n",
    "        # XGBoost\n",
    "        xgb = XGBClassifier(**xgb_params, enable_categorical=True)\n",
    "        xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        val_pred_xgb = xgb.predict_proba(X_val)[:, 1]\n",
    "        oof_preds_xgb[val_idx] = val_pred_xgb\n",
    "        test_preds_xgb += xgb.predict_proba(X_test_fold)[:, 1] / N_SPLITS\n",
    "        print(f\"XGB AUC: {roc_auc_score(y_val, val_pred_xgb):.5f}\")\n",
    "        \n",
    "        # LightGBM\n",
    "        lgb = LGBMClassifier(**lgb_params)\n",
    "        lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[])\n",
    "        val_pred_lgb = lgb.predict_proba(X_val)[:, 1]\n",
    "        oof_preds_lgb[val_idx] = val_pred_lgb\n",
    "        test_preds_lgb += lgb.predict_proba(X_test_fold)[:, 1] / N_SPLITS\n",
    "        print(f\"LGB AUC: {roc_auc_score(y_val, val_pred_lgb):.5f}\")\n",
    "        \n",
    "        # CatBoost\n",
    "        cat = CatBoostClassifier(**cat_params)\n",
    "        cat.fit(X_train, y_train, eval_set=(X_val, y_val), cat_features=CATS)\n",
    "        val_pred_cat = cat.predict_proba(X_val)[:, 1]\n",
    "        oof_preds_cat[val_idx] = val_pred_cat\n",
    "        test_preds_cat += cat.predict_proba(X_test_fold)[:, 1] / N_SPLITS\n",
    "        print(f\"CAT AUC: {roc_auc_score(y_val, val_pred_cat):.5f}\")\n",
    "        \n",
    "    return oof_preds_xgb, test_preds_xgb, oof_preds_lgb, test_preds_lgb, oof_preds_cat, test_preds_cat, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "if os.path.exists(ORIG_PATH):\n",
    "    print(f\"Found original dataset at {ORIG_PATH}\")\n",
    "    orig = pd.read_csv(ORIG_PATH)\n",
    "else:\n",
    "    print(\"Original dataset not found. Proceeding without it.\")\n",
    "    orig = None\n",
    "\n",
    "# 2. Feature Engineering\n",
    "train, test, FEATURES, CATS, INTER, ROUND, DIGIT = feature_engineering(train, test, orig)\n",
    "\n",
    "# 3. Train Models\n",
    "oof_xgb, pred_xgb, oof_lgb, pred_lgb, oof_cat, pred_cat, y = train_models(train, test, FEATURES, CATS, INTER, ROUND, DIGIT)\n",
    "\n",
    "# 4. Ensemble and Evaluation\n",
    "auc_xgb = roc_auc_score(y, oof_xgb)\n",
    "auc_lgb = roc_auc_score(y, oof_lgb)\n",
    "auc_cat = roc_auc_score(y, oof_cat)\n",
    "\n",
    "print(f\"\\nOverall XGB AUC: {auc_xgb:.5f}\")\n",
    "print(f\"Overall LGB AUC: {auc_lgb:.5f}\")\n",
    "print(f\"Overall CAT AUC: {auc_cat:.5f}\")\n",
    "\n",
    "# Simple Average\n",
    "ensemble_oof = (oof_xgb + oof_lgb + oof_cat) / 3\n",
    "ensemble_auc = roc_auc_score(y, ensemble_oof)\n",
    "print(f\"Ensemble (Average) AUC: {ensemble_auc:.5f}\")\n",
    "\n",
    "# Weighted Average\n",
    "total_auc = auc_xgb + auc_lgb + auc_cat\n",
    "w_xgb = auc_xgb / total_auc\n",
    "w_lgb = auc_lgb / total_auc\n",
    "w_cat = auc_cat / total_auc\n",
    "\n",
    "ensemble_oof_w = (oof_xgb * w_xgb + oof_lgb * w_lgb + oof_cat * w_cat)\n",
    "ensemble_auc_w = roc_auc_score(y, ensemble_oof_w)\n",
    "print(f\"Ensemble (Weighted) AUC: {ensemble_auc_w:.5f}\")\n",
    "\n",
    "# 5. Final Predictions & Submission\n",
    "final_preds = (pred_xgb + pred_lgb + pred_cat) / 3\n",
    "\n",
    "submission = pd.DataFrame({'id': test['id'], TARGET: final_preds})\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(f\"Submission saved to {SUBMISSION_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
